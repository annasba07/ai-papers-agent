# Data Ingestion Pipeline - Complete Guide

Complete guide to populating your AI papers knowledge graph with data from arXiv.

---

## Overview

The data ingestion pipeline handles the complete flow:

```
arXiv API â†’ Papers â†’ Concepts â†’ Embeddings â†’ Knowledge Graph
```

**What gets extracted:**
- ðŸ“„ Paper metadata (title, abstract, authors, dates, category)
- ðŸ·ï¸ Research concepts (techniques, architectures, datasets, tasks)
- ðŸ” Vector embeddings (for semantic similarity search)
- ðŸ“Š Full-text search vectors (for keyword search)

---

## Quick Start

### 1. Setup (One-time)

Ensure your environment is configured:

```bash
# Check .env file has required keys
cat backend/.env

# Should have:
# - SUPABASE_DATABASE_URL (Supabase connection)
# - OPENAI_API_KEY (for embeddings)
# - GEMINI_API_KEY (for concept extraction)
```

### 2. Initialize Database

```bash
cd backend
python -m app.db.init_db
```

### 3. Ingest Your First Papers

```bash
# Ingest 10 recent AI papers
python -m app.cli.ingest --category cs.AI --max 10

# Check what was ingested
python -m app.cli.ingest --stats
```

**That's it!** You now have papers in your knowledge graph.

---

## CLI Tool Usage

### Installation

```bash
cd backend
pip install -r requirements.txt
```

### Basic Commands

#### **Ingest by Category**

```bash
# Get 100 recent papers from cs.AI
python -m app.cli.ingest --category cs.AI --max 100

# Other popular categories
python -m app.cli.ingest --category cs.CV --max 100  # Computer Vision
python -m app.cli.ingest --category cs.LG --max 100  # Machine Learning
python -m app.cli.ingest --category cs.CL --max 100  # Computation & Language
python -m app.cli.ingest --category cs.RO --max 100  # Robotics
```

#### **Search and Ingest**

```bash
# Search for specific topics
python -m app.cli.ingest --query "attention mechanisms" --max 50
python -m app.cli.ingest --query "transformer architecture" --max 50
python -m app.cli.ingest --query "reinforcement learning" --max 50
```

#### **Ingest Recent Papers (Multiple Categories)**

```bash
# Get recent papers from AI, ML, and CV
python -m app.cli.ingest --recent --categories cs.AI cs.LG cs.CV --max-per 30

# This ingests up to 30 papers from each category (90 total)
```

#### **Bootstrap the Living Research Atlas (Last N Years)**

```bash
# Seed with the last 3 years of research using 3-month windows
python -m app.cli.ingest --bootstrap-atlas --years 3 --window-months 3 --max-window 200 --categories cs.AI cs.LG cs.CV cs.CL

# Notes:
# - Windows are processed chronologically (oldest â†’ newest) to avoid duplicates.
# - Embeddings are generated by default; add --no-embeddings to skip them for speed.
# - Use --extract-concepts if you want Gemini concept extraction during bootstrapping (slower).
```

#### **Ingest Specific Paper**

```bash
# Ingest a paper by arXiv ID
python -m app.cli.ingest --paper 2010.11929  # CLIP paper
python -m app.cli.ingest --paper 1706.03762  # Attention is All You Need
python -m app.cli.ingest --paper 2203.02155  # InstructGPT
```

---

## Advanced Usage

### Concept Extraction

By default, concepts are NOT extracted (faster ingestion). To extract concepts:

```bash
# Ingest with concept extraction (slower but richer data)
python -m app.cli.ingest --category cs.AI --max 50 --extract-concepts

# This will:
# 1. Fetch papers
# 2. Store in database
# 3. Extract concepts using Gemini AI
# 4. Store concepts and relationships
# 5. Generate embeddings
```

**Time estimate:**
- Without concepts: ~1-2 seconds per paper
- With concepts: ~5-10 seconds per paper (AI extraction)

### Skip Embeddings

If you want to ingest papers quickly and generate embeddings later:

```bash
# Ingest without embeddings
python -m app.cli.ingest --category cs.AI --max 1000 --no-embeddings

# Later, backfill embeddings
python -m app.cli.ingest --backfill-embeddings
```

### Backfill Operations

#### **Backfill Embeddings**

Generate embeddings for papers that don't have them:

```bash
# Backfill all papers without embeddings
python -m app.cli.ingest --backfill-embeddings

# Limit to 1000 papers
python -m app.cli.ingest --backfill-embeddings --max 1000

# Adjust batch size for faster processing
python -m app.cli.ingest --backfill-embeddings --batch-size 200
```

**Cost**: ~$0.0001 per paper (OpenAI text-embedding-3-small)

#### **Backfill Concepts**

Extract concepts for papers that don't have them:

```bash
# Backfill all papers without concepts
python -m app.cli.ingest --backfill-concepts

# Limit to 500 papers
python -m app.cli.ingest --backfill-concepts --max 500

# Smaller batch size (more API calls but safer)
python -m app.cli.ingest --backfill-concepts --batch-size 10
```

**Rate limiting**: Automatic delays between batches to respect Gemini API limits.

---

## Statistics and Monitoring

### View Statistics

```bash
python -m app.cli.ingest --stats
```

**Output example:**
```
ðŸ“š PAPERS:
   Total papers: 1,245
   Ingested (last 24h): 150

ðŸ“Š BY CATEGORY:
   cs.AI: 523 papers
   cs.LG: 412 papers
   cs.CV: 310 papers

ðŸ” EMBEDDINGS:
   Papers with embeddings: 1,200/1,245
   Coverage: 96.4%

ðŸ·ï¸ CONCEPTS:
   Total concepts: 1,834
   Papers with concepts: 980

ðŸ“ˆ TOP CONCEPTS:
   Transformer (architecture): 245 papers
   Attention Mechanism (technique): 198 papers
   Deep Learning (technique): 187 papers
   ...
```

### Monitor Progress

Watch the terminal output during ingestion:

```
======================================================================
  Ingesting Papers from Category: cs.AI
======================================================================

Fetched 100 papers from arXiv
Stored 95 new papers (5 duplicates)
Generating embeddings for new papers...
Successfully embedded 95/95 papers

ðŸ“Š Ingestion Complete - cs.AI:
--------------------------------------------------
  fetched: 100
  stored: 95
  duplicates: 5
  embeddings_generated: 95
  concepts_extracted: 0
  errors: 0
--------------------------------------------------
```

---

## Common Workflows

### Daily Update Workflow

Run this daily to stay current:

```bash
#!/bin/bash
# daily_update.sh

cd backend

# Ingest recent papers from top categories
python -m app.cli.ingest --recent \
  --categories cs.AI cs.LG cs.CV cs.CL cs.RO \
  --max-per 20

# Check stats
python -m app.cli.ingest --stats
```

Make it executable and schedule with cron:
```bash
chmod +x daily_update.sh

# Run daily at 9 AM
crontab -e
# Add: 0 9 * * * /path/to/daily_update.sh
```

### Initial Large Import

For first-time setup with lots of papers:

```bash
# Step 1: Ingest papers quickly (no concepts, no embeddings)
python -m app.cli.ingest --category cs.AI --max 5000 --no-embeddings

# Step 2: Generate embeddings in batches
python -m app.cli.ingest --backfill-embeddings --batch-size 200

# Step 3: Extract concepts for subset (expensive)
python -m app.cli.ingest --backfill-concepts --max 1000 --batch-size 10

# Step 4: Check coverage
python -m app.cli.ingest --stats
```

### Research-Specific Import

Building a dataset for specific research:

```bash
# Import all papers about your research topic
python -m app.cli.ingest --query "graph neural networks" --max 500 --extract-concepts
python -m app.cli.ingest --query "knowledge graphs" --max 500 --extract-concepts
python -m app.cli.ingest --query "GNN explainability" --max 200 --extract-concepts

# Check what concepts were extracted
python -m app.cli.ingest --stats
```

---

## Performance

### Ingestion Speed

| Papers | Without Concepts | With Concepts |
|--------|-----------------|---------------|
| 10 | ~20 seconds | ~60 seconds |
| 100 | ~3 minutes | ~15 minutes |
| 1,000 | ~30 minutes | ~2.5 hours |
| 10,000 | ~5 hours | ~25 hours |

**Bottlenecks:**
- Concept extraction (AI calls to Gemini)
- Embedding generation (API calls to OpenAI)
- Network latency (arXiv API)

**Tips for speed:**
1. Use `--no-embeddings` and backfill later
2. Skip concept extraction initially
3. Use larger batch sizes (if API allows)
4. Run overnight for large imports

### Cost Estimates

**Embeddings (OpenAI text-embedding-3-small):**
- $0.0001 per paper
- 1,000 papers = ~$0.10
- 10,000 papers = ~$1.00
- 100,000 papers = ~$10.00

**Concept Extraction (Gemini):**
- Free tier: 60 requests/minute
- Paid: Depends on usage
- Recommend using sparingly for large datasets

**Total for 10,000 papers:**
- Embeddings: ~$1.00
- Concepts (if using Gemini paid): Varies
- **Total**: $1-5 typically

Very affordable for a powerful research tool!

---

## Troubleshooting

### Error: "No module named 'app'"

**Solution**: Make sure you're running from the backend directory:

```bash
cd backend
python -m app.cli.ingest --help
```

### Error: "Database connection failed"

**Solution**: Check your Supabase connection string:

```bash
# Test connection
python -c "import os; from dotenv import load_dotenv; load_dotenv(); print(os.getenv('SUPABASE_DATABASE_URL'))"

# Should print your connection string
```

### Error: "OPENAI_API_KEY not found"

**Solution**: Add your OpenAI API key to `.env`:

```bash
echo "OPENAI_API_KEY=sk-your_key_here" >> backend/.env
```

### Slow Ingestion

**Causes:**
- Gemini API rate limiting (with `--extract-concepts`)
- OpenAI API rate limiting (embedding generation)
- Network latency

**Solutions:**
1. Use `--no-embeddings` and backfill later
2. Skip concept extraction for bulk imports
3. Reduce batch size: `--batch-size 50`
4. Run overnight for large datasets

### Duplicate Papers

The system automatically detects duplicates by arXiv ID. You'll see:

```
duplicates: 5
```

This is normal and expected - the system won't create duplicate entries.

### Rate Limiting

If you hit rate limits:

**arXiv API:**
- Limit: ~1 request per 3 seconds
- Solution: Automatic delays built-in

**OpenAI API:**
- Free tier: 3 RPM
- Solution: Reduce batch size or upgrade tier

**Gemini API:**
- Free tier: 60 RPM
- Solution: Add delays or use paid tier

---

## API Integration (Alternative to CLI)

You can also use the ingestion service programmatically:

```python
import asyncio
from app.services.ingestion_service import get_ingestion_service

async def main():
    service = get_ingestion_service()

    # Ingest papers
    stats = await service.ingest_by_category(
        category="cs.AI",
        max_results=100,
        generate_embeddings=True
    )

    print(f"Ingested {stats['stored']} papers")

asyncio.run(main())
```

Or create an API endpoint:

```python
# In backend/app/api/v1/endpoints/ingestion.py (you can create this)

@router.post("/ingest/category")
async def ingest_category(category: str, max_results: int = 100):
    service = get_ingestion_service()
    stats = await service.ingest_by_category(
        category=category,
        max_results=max_results
    )
    return stats
```

---

## Best Practices

### 1. Start Small

```bash
# Test with 10 papers first
python -m app.cli.ingest --category cs.AI --max 10

# Check everything works
python -m app.cli.ingest --stats

# Then scale up
python -m app.cli.ingest --category cs.AI --max 1000
```

### 2. Separate Ingestion and Processing

```bash
# Fast ingestion
python -m app.cli.ingest --category cs.AI --max 5000 --no-embeddings

# Process later in background
nohup python -m app.cli.ingest --backfill-embeddings > embeddings.log 2>&1 &
nohup python -m app.cli.ingest --backfill-concepts --max 1000 > concepts.log 2>&1 &
```

### 3. Monitor Your Database

```bash
# Check database size in Supabase dashboard
# Free tier: 500MB limit

# If approaching limit:
# - Delete old papers
# - Upgrade to Pro tier ($25/month for 8GB)
```

### 4. Version Your Data

```bash
# Backup before large operations
pg_dump $SUPABASE_DATABASE_URL > backup_$(date +%Y%m%d).sql

# Or use Supabase dashboard -> Database -> Backups
```

---

## Next Steps

After ingesting papers:

1. **Test the API**: Try the knowledge graph endpoints
   ```bash
   curl "http://localhost:8000/api/v1/knowledge-graph/papers/latest?category=cs.AI&limit=10"
   ```

2. **Build Frontend**: Create UI components to visualize your data

3. **Explore Queries**: Try semantic search, citation networks, trending concepts

4. **Automate**: Set up daily cron jobs to stay current

---

## Architecture

```
CLI Tool (ingest.py)
    â†“
Ingestion Service
    â†“
    â”œâ”€â†’ arXiv Service â”€â”€â†’ Fetch papers
    â”œâ”€â†’ Database â”€â”€â†’ Store papers
    â”œâ”€â†’ Concept Extraction Service â”€â”€â†’ Extract & store concepts
    â””â”€â†’ Embedding Service â”€â”€â†’ Generate & store embeddings
```

**Services:**
- `ingestion_service.py` - Orchestrates the pipeline
- `arxiv_service.py` - Fetches from arXiv API
- `concept_extraction_service.py` - Extracts concepts with Gemini
- `embedding_service.py` - Generates embeddings with OpenAI

**Database Tables:**
- `papers` - Paper metadata + embeddings
- `concepts` - Extracted concepts
- `paper_concepts` - Relationships with relevance scores

---

## Support

For issues:
1. Check `--stats` to see current state
2. Look at error messages (full stack traces shown)
3. Verify API keys in `.env`
4. Check Supabase dashboard for database status

---

**Ready to build the largest AI research repository!** ðŸš€

Start with: `python -m app.cli.ingest --category cs.AI --max 10`
