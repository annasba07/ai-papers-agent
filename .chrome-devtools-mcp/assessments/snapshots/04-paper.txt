uid=16_0 RootWebArea "AI Paper Atlas | Research Intelligence Platform" url="http://localhost:3000/explore"
  uid=16_1 banner
    uid=16_2 link "Paper Atlas" url="http://localhost:3000/explore"
      uid=16_3 StaticText "Paper Atlas"
    uid=16_4 navigation
      uid=16_5 link "Explore" url="http://localhost:3000/explore"
        uid=16_6 StaticText "Explore"
      uid=16_7 link "Generate" url="http://localhost:3000/generate"
        uid=16_8 StaticText "Generate"
  uid=16_9 complementary
    uid=16_10 heading "Filters" level="2"
    uid=16_11 StaticText "6"
    uid=16_12 StaticText " papers"
    uid=16_13 heading "QUICK FILTERS" level="3"
    uid=16_14 button "Has Code"
    uid=16_15 button "High Impact (7+)"
    uid=16_16 heading "CATEGORY" level="3"
    uid=16_17 button "Artificial Intelligence"
    uid=16_18 button "Machine Learning"
    uid=16_19 button "Computation & Language"
    uid=16_20 button "Computer Vision"
    uid=16_21 button "Neural & Evolutionary"
    uid=16_22 button "Robotics"
    uid=16_23 button "Statistics ML"
    uid=16_24 heading "DIFFICULTY" level="3"
    uid=16_25 button "Beginner"
    uid=16_26 button "Intermediate"
    uid=16_27 button "Advanced"
    uid=16_28 button "Expert"
    uid=16_29 heading "TRENDING TOPICS" level="3"
    uid=16_30 StaticText "LLM Agents"
    uid=16_31 StaticText "Mixture of Experts"
    uid=16_32 StaticText "RLHF"
    uid=16_33 StaticText "Diffusion"
    uid=16_34 StaticText "RAG"
    uid=16_35 StaticText "Papers indexed"
    uid=16_36 StaticText "6"
    uid=16_37 StaticText "Updated"
    uid=16_38 StaticText "Just now"
  uid=16_39 main
    uid=16_40 textbox "Describe what you're researching (e.g., 'efficient attention for mobile deployment')..." value="efficient attention mechanisms"
    uid=16_41 button "Ask Advisor"
    uid=16_42 StaticText "6 results (8022ms)"
    uid=16_43 StaticText "Smart Results"
    uid=16_44 StaticText "âœ¦"
    uid=16_45 StaticText "AI-POWERED"
    uid=16_46 StaticText "8020"
    uid=16_47 StaticText "ms"
    uid=16_48 link "Attentions Under the Microscope: A Comparative Study of Resource Utilization for Variants of Self-Attention" url="http://arxiv.org/abs/2507.07247v1"
      uid=16_49 StaticText "Attentions Under the Microscope: A Comparative Study of Resource Utilization for Variants of Self-Attention"
    uid=16_50 StaticText "Invalid Date"
    uid=16_51 button "Collapse" focusable focused
    uid=16_52 button "Summary"
    uid=16_53 button "Related Papers"
    uid=16_54 button "Benchmarks"
    uid=16_55 heading "Full Abstract" level="4"
    uid=16_56 StaticText "As large language models (LLMs) and visual language models (VLMs) grow in scale and application, attention mechanisms have become a central computational bottleneck due to their high memory and time complexity. While many efficient attention variants have been proposed, there remains a lack of rigorous evaluation on their actual energy usage and hardware resource demands during training. In this work, we benchmark eight attention mechanisms in training GPT-2 architecture, measuring key metrics including training time, GPU memory usage, FLOPS, CPU usage, and power consumption. Our results reveal that attention mechanisms with optimized kernel implementations, including Flash Attention, Locality-Sensitive Hashing (LSH) Attention, and Multi-Head Latent Attention (MLA), achieve the best energy efficiency. We further show that lower GPU power alone does not guarantee reduced energy use, as training time plays an equally important role. Our study highlights the importance of energy-aware benchmarking in attention design and provides a practical insight for selecting resource-efficient mechanisms. All our codes are available at GitHub."
    uid=16_57 link "Read on arXiv" url="http://arxiv.org/abs/2507.07247v1"
      uid=16_58 StaticText "Read on arXiv"
    uid=16_59 link "Generate Code" url="http://localhost:3000/generate?paper=2507.07247"
      uid=16_60 StaticText "Generate Code"
    uid=16_61 link "LSNet: See Large, Focus Small" url="http://arxiv.org/abs/2503.23135v1"
      uid=16_62 StaticText "LSNet: See Large, Focus Small"
    uid=16_63 StaticText "TL;DR"
    uid=16_64 StaticText "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet..."
    uid=16_65 StaticText "Invalid Date"
    uid=16_66 button "Expand"
    uid=16_67 link "Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing" url="http://arxiv.org/abs/2411.19460v1"
      uid=16_68 StaticText "Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing"
    uid=16_69 StaticText "TL;DR"
    uid=16_70 StaticText "With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic inc..."
    uid=16_71 StaticText "Invalid Date"
    uid=16_72 button "Expand"
    uid=16_73 link "Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration" url="http://arxiv.org/abs/2502.10699v2"
      uid=16_74 StaticText "Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration"
    uid=16_75 StaticText "TL;DR"
    uid=16_76 StaticText "Contextual memory integration remains a high challenge in the development of language models, particularly in tasks that require maintaining coherence..."
    uid=16_77 StaticText "Invalid Date"
    uid=16_78 button "Expand"
    uid=16_79 link "AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning" url="http://arxiv.org/abs/2505.12782v1"
      uid=16_80 StaticText "AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning"
    uid=16_81 StaticText "TL;DR"
    uid=16_82 StaticText "Large Multimodal Models (LMMs) have become a pivotal research focus in deep learning, demonstrating remarkable capabilities in 3D scene understanding...."
    uid=16_83 StaticText "Invalid Date"
    uid=16_84 button "Expand"
    uid=16_85 link "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference" url="http://arxiv.org/abs/2503.23956v3"
      uid=16_86 StaticText "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference"
    uid=16_87 StaticText "TL;DR"
    uid=16_88 StaticText "Recent advancements in Large Visual Language Models (LVLMs) have gained significant attention due to their remarkable reasoning capabilities and profi..."
    uid=16_89 StaticText "Invalid Date"
    uid=16_90 button "Expand"
    uid=16_91 heading "Trending Now" level="3"
    uid=16_92 button "Hot Topics"
    uid=16_93 button "Rising"
    uid=16_94 button "Emerging"
    uid=16_95 StaticText "1"
    uid=16_96 StaticText "Dropout"
    uid=16_97 StaticText "papers"
    uid=16_98 StaticText "+"
    uid=16_99 StaticText "29900"
    uid=16_100 StaticText "%"
    uid=16_101 StaticText "2"
    uid=16_102 StaticText "Ssm"
    uid=16_103 StaticText "papers"
    uid=16_104 StaticText "+"
    uid=16_105 StaticText "12841"
    uid=16_106 StaticText "%"
    uid=16_107 StaticText "3"
    uid=16_108 StaticText "Peft"
    uid=16_109 StaticText "papers"
    uid=16_110 StaticText "+"
    uid=16_111 StaticText "10456"
    uid=16_112 StaticText "%"
    uid=16_113 StaticText "4"
    uid=16_114 StaticText "Rlhf"
    uid=16_115 StaticText "papers"
    uid=16_116 StaticText "+"
    uid=16_117 StaticText "9900"
    uid=16_118 StaticText "%"
    uid=16_119 StaticText "5"
    uid=16_120 StaticText "Distillation"
    uid=16_121 StaticText "papers"
    uid=16_122 StaticText "+"
    uid=16_123 StaticText "9255"
    uid=16_124 StaticText "%"
    uid=16_125 StaticText "6"
    uid=16_126 StaticText "Diffusion"
    uid=16_127 StaticText "papers"
    uid=16_128 StaticText "+"
    uid=16_129 StaticText "8628"
    uid=16_130 StaticText "%"
  uid=16_131 button "Open Next.js Dev Tools" expandable haspopup="menu"
