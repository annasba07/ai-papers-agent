uid=10_0 RootWebArea "AI Paper Atlas | Research Intelligence Platform" url="http://localhost:3000/explore"
  uid=10_1 banner
    uid=10_2 link "Paper Atlas" url="http://localhost:3000/explore"
      uid=10_3 StaticText "Paper Atlas"
    uid=10_4 navigation
      uid=10_5 link "Explore" url="http://localhost:3000/explore"
        uid=10_6 StaticText "Explore"
      uid=10_7 link "Generate" url="http://localhost:3000/generate"
        uid=10_8 StaticText "Generate"
  uid=10_9 complementary
    uid=10_10 heading "Filters" level="2"
    uid=10_11 StaticText "6"
    uid=10_12 StaticText " papers"
    uid=10_13 heading "QUICK FILTERS" level="3"
    uid=10_14 button "Has Code"
    uid=10_15 button "High Impact (7+)"
    uid=10_16 heading "CATEGORY" level="3"
    uid=10_17 button "Artificial Intelligence"
    uid=10_18 button "Machine Learning"
    uid=10_19 button "Computation & Language"
    uid=10_20 button "Computer Vision"
    uid=10_21 button "Neural & Evolutionary"
    uid=10_22 button "Robotics"
    uid=10_23 button "Statistics ML"
    uid=10_24 heading "DIFFICULTY" level="3"
    uid=10_25 button "Beginner"
    uid=10_26 button "Intermediate"
    uid=10_27 button "Advanced"
    uid=10_28 button "Expert"
    uid=10_29 heading "TRENDING TOPICS" level="3"
    uid=10_30 StaticText "LLM Agents"
    uid=10_31 StaticText "Mixture of Experts"
    uid=10_32 StaticText "RLHF"
    uid=10_33 StaticText "Diffusion"
    uid=10_34 StaticText "RAG"
    uid=10_35 StaticText "Papers indexed"
    uid=10_36 StaticText "6"
    uid=10_37 StaticText "Updated"
    uid=10_38 StaticText "Just now"
  uid=10_39 main
    uid=10_40 textbox "Describe what you're researching (e.g., 'efficient attention for mobile deployment')..." value="efficient attention mechanisms"
    uid=10_41 button "Ask Advisor"
    uid=10_42 StaticText "6 results (8022ms)"
    uid=10_43 StaticText "Smart Results"
    uid=10_44 StaticText "✦"
    uid=10_45 StaticText "AI-POWERED"
    uid=10_46 StaticText "8020"
    uid=10_47 StaticText "ms"
    uid=10_48 link "Attentions Under the Microscope: A Comparative Study of Resource Utilization for Variants of Self-Attention" url="http://arxiv.org/abs/2507.07247v1"
      uid=10_49 StaticText "Attentions Under the Microscope: A Comparative Study of Resource Utilization for Variants of Self-Attention"
    uid=10_50 StaticText "TL;DR"
    uid=10_51 StaticText "As large language models (LLMs) and visual language models (VLMs) grow in scale and application, attention mechanisms have become a central computatio..."
    uid=10_52 StaticText "Invalid Date"
    uid=10_53 button "Expand"
    uid=10_54 link "LSNet: See Large, Focus Small" url="http://arxiv.org/abs/2503.23135v1"
      uid=10_55 StaticText "LSNet: See Large, Focus Small"
    uid=10_56 StaticText "TL;DR"
    uid=10_57 StaticText "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet..."
    uid=10_58 StaticText "Invalid Date"
    uid=10_59 button "Expand"
    uid=10_60 link "Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing" url="http://arxiv.org/abs/2411.19460v1"
      uid=10_61 StaticText "Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing"
    uid=10_62 StaticText "TL;DR"
    uid=10_63 StaticText "With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic inc..."
    uid=10_64 StaticText "Invalid Date"
    uid=10_65 button "Expand"
    uid=10_66 link "Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration" url="http://arxiv.org/abs/2502.10699v2"
      uid=10_67 StaticText "Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration"
    uid=10_68 StaticText "TL;DR"
    uid=10_69 StaticText "Contextual memory integration remains a high challenge in the development of language models, particularly in tasks that require maintaining coherence..."
    uid=10_70 StaticText "Invalid Date"
    uid=10_71 button "Expand"
    uid=10_72 link "AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning" url="http://arxiv.org/abs/2505.12782v1"
      uid=10_73 StaticText "AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning"
    uid=10_74 StaticText "TL;DR"
    uid=10_75 StaticText "Large Multimodal Models (LMMs) have become a pivotal research focus in deep learning, demonstrating remarkable capabilities in 3D scene understanding...."
    uid=10_76 StaticText "Invalid Date"
    uid=10_77 button "Expand"
    uid=10_78 link "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference" url="http://arxiv.org/abs/2503.23956v3"
      uid=10_79 StaticText "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference"
    uid=10_80 StaticText "TL;DR"
    uid=10_81 StaticText "Recent advancements in Large Visual Language Models (LVLMs) have gained significant attention due to their remarkable reasoning capabilities and profi..."
    uid=10_82 StaticText "Invalid Date"
    uid=10_83 button "Expand"
    uid=10_84 heading "Trending Now" level="3"
    uid=10_85 button "Hot Topics"
    uid=10_86 button "Rising"
    uid=10_87 button "Emerging"
    uid=10_88 StaticText "1"
    uid=10_89 StaticText "Dropout"
    uid=10_90 StaticText "papers"
    uid=10_91 StaticText "+"
    uid=10_92 StaticText "29900"
    uid=10_93 StaticText "%"
    uid=10_94 StaticText "2"
    uid=10_95 StaticText "Ssm"
    uid=10_96 StaticText "papers"
    uid=10_97 StaticText "+"
    uid=10_98 StaticText "12841"
    uid=10_99 StaticText "%"
    uid=10_100 StaticText "3"
    uid=10_101 StaticText "Peft"
    uid=10_102 StaticText "papers"
    uid=10_103 StaticText "+"
    uid=10_104 StaticText "10456"
    uid=10_105 StaticText "%"
    uid=10_106 StaticText "4"
    uid=10_107 StaticText "Rlhf"
    uid=10_108 StaticText "papers"
    uid=10_109 StaticText "+"
    uid=10_110 StaticText "9900"
    uid=10_111 StaticText "%"
    uid=10_112 StaticText "5"
    uid=10_113 StaticText "Distillation"
    uid=10_114 StaticText "papers"
    uid=10_115 StaticText "+"
    uid=10_116 StaticText "9255"
    uid=10_117 StaticText "%"
    uid=10_118 StaticText "6"
    uid=10_119 StaticText "Diffusion"
    uid=10_120 StaticText "papers"
    uid=10_121 StaticText "+"
    uid=10_122 StaticText "8628"
    uid=10_123 StaticText "%"
  uid=10_124 StaticText "Research Advisor"
  uid=10_125 button "Close"
  uid=10_126 StaticText "Hi! I'm your Research Advisor. Describe what you're working on or what problem you're trying to solve, and I'll help you find relevant papers and techniques."
  uid=10_127 StaticText "I'm researching efficient transformers for mobile deployment"
  uid=10_128 StaticText "Contextual synthesis temporarily unavailable. Here is a quick brief of promising papers: - AutoTailor: Automatic and Efficient Adaptive Model Deployment for Diverse Edge Devices — inspect its methodology for actionable leads. - Reflection Removal through Efficient Adaptation of Diffusion Transformers — inspect its methodology for actionable leads. - Constructing Efficient Fact-Storing MLPs for Transformers — inspect its methodology for actionable leads."
  uid=10_129 StaticText "RELEVANT PAPERS:"
  uid=10_130 link "AutoTailor: Automatic and Efficient Adaptive Model Deployment for Diverse Edge Devices" url="https://arxiv.org/abs/2511.22355v1"
    uid=10_131 StaticText "AutoTailor: Automatic and Efficient Adaptive Model Deployment for Diverse Edge Devices"
  uid=10_132 link "Reflection Removal through Efficient Adaptation of Diffusion Transformers" url="https://arxiv.org/abs/2512.05000v1"
    uid=10_133 StaticText "Reflection Removal through Efficient Adaptation of Diffusion Transformers"
  uid=10_134 link "Constructing Efficient Fact-Storing MLPs for Transformers" url="https://arxiv.org/abs/2512.00207v1"
    uid=10_135 StaticText "Constructing Efficient Fact-Storing MLPs for Transformers"
  uid=10_136 link "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers" url="https://arxiv.org/abs/2512.03870v1"
    uid=10_137 StaticText "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers"
  uid=10_138 link "Energy-Aware Resource Allocation for Multi-Operator Cell-Free Massive MIMO in V-CRAN Architectures" url="http://arxiv.org/abs/2507.21644v2"
    uid=10_139 StaticText "Energy-Aware Resource Allocation for Multi-Operator Cell-Free Massive MIMO in V-CRAN Architectures"
  uid=10_140 button "Find papers that cite these works"
  uid=10_141 button "What are alternative approaches to this problem?"
  uid=10_142 button "Show me implementation code for these techniques"
  uid=10_143 textbox "Describe your research problem..."
  uid=10_144 button disableable disabled
  uid=10_145 button "Open Next.js Dev Tools" expandable haspopup="menu"
